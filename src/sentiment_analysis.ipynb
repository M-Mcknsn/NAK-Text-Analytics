{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Sentiment Analysis\n",
    "*Jannik Labs, 2023*\n",
    "*Master \"Applied Data Science\" @ Nordakademie*\n",
    "*Modul: Text Analytics*"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Intro\n",
    "\n",
    "Different approaches/methods or respective libraries possible for mapping sentiment values to the \"cleaned_articles_v1.csv\". Each coming in with its own characteristics, weaknesses and strengths:\n",
    "\n",
    "1. **TextBlob**\n",
    "- beginner-friendly and relatively easy to set up\n",
    "- pre-trained models for sentiment analysis\n",
    "- suitable for basic sentiment analysis tasks\n",
    "- limited customization options\n",
    "\n",
    "2. **NLTK Toolkit** - in general\n",
    "- wide range of tools for text processing, including sentiment analysis\n",
    "- more flexibility for creating custom sentiment analysis models\n",
    "\n",
    "    - **VADER**-- (**V**alence **A**ware **D**ictionary and s**E**ntiment **R**easoner) (PART OF NLTK)\n",
    "    - specific for Social Media, especially for informal texts like tweets and social media posts\n",
    "    - lexicon-based and comes with a pre-trained sentiment lexicon\n",
    "    - good for real-time sentiment analysis\n",
    "\n",
    "3. **Scikit-learn**\n",
    "- can be used to build a custom sentiment analysis model using machine learning\n",
    "- allows engineering of custom features for sentiment analysis\n",
    "\n",
    "4. **Transformer Models like BERT or GPT**\n",
    "- State-of-the-Art Models: Utilizes pre-trained transformer-based models (e.g., BERT, GPT) for sentiment analysis\n",
    "- High Performance: Provides state-of-the-art performance in NLP tasks\n",
    "- requires significant computational resources and large amounts of data\n",
    "\n",
    "### Decision\n",
    "For our use case custom sentiment models with special features are rather out of scope. Neither do we mainly analyse really short texts as found on Social Media (tweets on X etc.). Therefore, in order to ensure comparisons of different tools/libraries, here we will use:\n",
    "\n",
    "- TextBlob\n",
    "- NLTK Toolkit (using VADER) (still used for a comparison)\n",
    "- BERT\n",
    "\n",
    "As ressource heaviness increases these tools will be used in this exact order."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Sentiment Analyses"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### General pre-work"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\jannik.labs\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "# Overall needed\n",
    "import pandas as pd # to work with data frames\n",
    "import numpy as np\n",
    "\n",
    "# Needed for sentiment analyses and different approaches\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.nn.functional import softmax\n",
    "\n",
    "# Needed for visualisations and smaller functions such as time tracking or saving files\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import os # to define a dedicated output folder\n",
    "import time # to track the run time for different approaches\n",
    "import random\n",
    "\n",
    "output_folder = 'data/sentiment_results' # refer to a new folder to store only sentiment result files\n",
    "\n",
    "# check if folder can be found, else create it\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "outputs": [],
   "source": [
    "# Load the CSV file from the data cleaning part\n",
    "\n",
    "csv_file_path = 'data/transfer/cleaned_articles_v1.csv' # define the path of the input csv file\n",
    "\n",
    "# Load the data set, with the option of defining only a subset of the data (first x rows as an example). This way we can test ALL our approaches first and see some first results. Just use parameter \"nrows=x\". Total row number of the input is around 23.500.\n",
    "df = pd.read_csv(csv_file_path, encoding='utf-8', quoting=csv.QUOTE_ALL, nrows=500) # ensuring the right encoding as in the csv file we still encounter incorrectly encoded special characters\n",
    "total_rows = len(df) #define total rows\n",
    "\n",
    "# Create separate data frame copies for the sentiment analysis approaches\n",
    "df_textblob = df.copy()       # For TextBlob approach\n",
    "df_nltk = df.copy()           # For NLTK approach\n",
    "df_bert_large = df.copy()     # for BERT approach with BERT-large\n",
    "df_bert_roberta = df.copy()   # for BERT approach with RoBERTa\n",
    "df_bert_longform = df.copy()  # for BERT approach with Longform"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1st Approach with TextBlob"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'sentiment_score' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mUnboundLocalError\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_10880\\1429711991.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     25\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     26\u001B[0m \u001B[1;31m# Applying the defined sentiment analysis function\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 27\u001B[1;33m \u001B[0mdf_textblob\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'TextBlob_Sentiment_Label'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'TextBlob_Sentiment_Score'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mdf_textblob\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'Cleaned_Text'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mapply\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0manalyze_sentiment\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mapply\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpd\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mSeries\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     28\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     29\u001B[0m \u001B[1;31m# Calculate and print the elapsed time\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001B[0m in \u001B[0;36mapply\u001B[1;34m(self, func, convert_dtype, args, **kwargs)\u001B[0m\n\u001B[0;32m   4431\u001B[0m         \u001B[0mdtype\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mfloat64\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   4432\u001B[0m         \"\"\"\n\u001B[1;32m-> 4433\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mSeriesApply\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfunc\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mconvert_dtype\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mapply\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   4434\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   4435\u001B[0m     def _reduce(\n",
      "\u001B[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\apply.py\u001B[0m in \u001B[0;36mapply\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1086\u001B[0m             \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mapply_str\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1087\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1088\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mapply_standard\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1089\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1090\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0magg\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\apply.py\u001B[0m in \u001B[0;36mapply_standard\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1141\u001B[0m                 \u001B[1;31m# List[Union[Callable[..., Any], str]]]]]\"; expected\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1142\u001B[0m                 \u001B[1;31m# \"Callable[[Any], Any]\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1143\u001B[1;33m                 mapped = lib.map_infer(\n\u001B[0m\u001B[0;32m   1144\u001B[0m                     \u001B[0mvalues\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1145\u001B[0m                     \u001B[0mf\u001B[0m\u001B[1;33m,\u001B[0m  \u001B[1;31m# type: ignore[arg-type]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\_libs\\lib.pyx\u001B[0m in \u001B[0;36mpandas._libs.lib.map_infer\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_10880\\1429711991.py\u001B[0m in \u001B[0;36manalyze_sentiment\u001B[1;34m(text)\u001B[0m\n\u001B[0;32m     11\u001B[0m     \u001B[1;32mglobal\u001B[0m \u001B[0mcurrent_row\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     12\u001B[0m     \u001B[0mcurrent_row\u001B[0m \u001B[1;33m+=\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 13\u001B[1;33m     \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34mf\"Analyzing text number {current_row} out of {total_rows} rows in the input file. Sentiment Score = {sentiment_score}.\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     14\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     15\u001B[0m     \u001B[0manalysis\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mTextBlob\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtext\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mUnboundLocalError\u001B[0m: local variable 'sentiment_score' referenced before assignment"
     ]
    }
   ],
   "source": [
    "used_approach = \"TextBlob\"  # Setting it for later reference\n",
    "\n",
    "# Initialize a counter for tracking progress\n",
    "current_row = 0\n",
    "\n",
    "# Record the start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Defining function for sentiment analysis\n",
    "def analyze_sentiment(text):\n",
    "    global current_row\n",
    "    current_row += 1\n",
    "    print(f\"Analyzing text number {current_row} out of {total_rows} rows in the input file.\")\n",
    "\n",
    "    analysis = TextBlob(text)\n",
    "    sentiment_score = analysis.sentiment.polarity\n",
    "    if sentiment_score > 0:\n",
    "        sentiment_label = 'Positive'\n",
    "    elif sentiment_score == 0:\n",
    "        sentiment_label = 'Neutral'\n",
    "    else:\n",
    "        sentiment_label = 'Negative'\n",
    "\n",
    "    return sentiment_label, sentiment_score  # Return both label and score\n",
    "\n",
    "# Applying the defined sentiment analysis function\n",
    "df_textblob[['TextBlob_Sentiment_Label', 'TextBlob_Sentiment_Score']] = df_textblob['Cleaned_Text'].apply(analyze_sentiment).apply(pd.Series)\n",
    "\n",
    "# Calculate and print the elapsed time\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(\"-\"*100)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# End information about the run\n",
    "print(f\"The used approach #1 was {used_approach}.\")\n",
    "print(f\"Elapsed time: {elapsed_time:.0f} seconds.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Saving the output in two file formats in dedicated files for only this current approach in the correct repository folder\n",
    "\n",
    "# Save the results to an Excel file\n",
    "excel_output_file_textblob = os.path.join(output_folder, 'sentiment_results_textblob.xlsx')\n",
    "df_textblob.to_excel(excel_output_file_textblob, index=False)\n",
    "\n",
    "# Save the results to a csv file with the same name\n",
    "csv_output_file_textblob = os.path.join(output_folder, 'sentiment_results_textblob.csv')\n",
    "df_textblob.to_csv(csv_output_file_textblob, index=False, encoding='utf-8', quoting=csv.QUOTE_ALL)\n",
    "\n",
    "# Print final info\n",
    "print(f\"Results saved as {csv_output_file_textblob} and {excel_output_file_textblob}.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Explorative Analysis of TextBlob"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Calculate and print statistics\n",
    "\n",
    "print(\"Statistics for TextBlob_Sentiment_Score:\")\n",
    "\n",
    "print(\"\\nAverage Score:\")\n",
    "print(df_textblob['TextBlob_Sentiment_Score'].mean())\n",
    "print(\"\\nMinimum Score:\")\n",
    "print(df_textblob['TextBlob_Sentiment_Score'].min())\n",
    "print(\"\\nMaximum Score:\")\n",
    "print(df_textblob['TextBlob_Sentiment_Score'].max())\n",
    "print(\"\\nStandard Deviation:\")\n",
    "print(df_textblob['TextBlob_Sentiment_Score'].std())\n",
    "\n",
    "# statistics for 'TextBlob_Sentiment_Label'\n",
    "print(\"\\nStatistics for TextBlob_Sentiment_Label:\")\n",
    "print(\"Count of values:\")\n",
    "print(df_textblob['TextBlob_Sentiment_Label'].value_counts())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Check some results of TextBlob including all columns\n",
    "\n",
    "# Randomly sample 5 positive entries\n",
    "positive_entries = df_textblob[df_textblob['TextBlob_Sentiment_Score'] > 0].sample(5, random_state=42)\n",
    "\n",
    "# Randomly sample 5 negative entries\n",
    "negative_entries = df_textblob[df_textblob['TextBlob_Sentiment_Score'] < 0].sample(5, random_state=42)\n",
    "\n",
    "# Concatenate the positive and negative entries along with all columns\n",
    "sampled_entries = pd.concat([positive_entries, negative_entries])\n",
    "\n",
    "# Display all columns for the sampled entries\n",
    "print(sampled_entries)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create a histogram for sentiment scores in df_textblob\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.histplot(df_textblob['TextBlob_Sentiment_Score'], bins=20, color='skyblue', edgecolor='black')\n",
    "plt.title('Distribution of TextBlob Sentiment Scores')\n",
    "plt.xlabel('Sentiment Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot regarding the labels\n",
    "\n",
    "# Count the occurrences of each sentiment label\n",
    "sentiment_counts = df_textblob['TextBlob_Sentiment_Label'].value_counts()\n",
    "\n",
    "# Define the order of sentiment categories\n",
    "sentiment_order = ['Positive', 'Neutral', 'Negative']\n",
    "\n",
    "# Define colors for the sentiment categories\n",
    "colors = {'Positive': 'green', 'Neutral': 'gray', 'Negative': 'red'}\n",
    "\n",
    "# Create a bar plot (histogram) for sentiment distribution with correct colors and order\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.set_palette([colors[s] for s in sentiment_order])\n",
    "ax = sns.barplot(x=sentiment_counts.index, y=sentiment_counts.values, order=sentiment_order)\n",
    "plt.title('Sentiment Distribution - TextBlob')\n",
    "plt.xlabel('Sentiment Label - categorised with thresholds')\n",
    "plt.ylabel('Count of Sentiment')\n",
    "\n",
    "# Add labels with integer values of counts to the bars\n",
    "for p in ax.patches:\n",
    "    ax.annotate(f'{int(p.get_height())}', (p.get_x() + p.get_width() / 2., p.get_height()), ha='center', va='center', fontsize=12, color='black', xytext=(0, 5), textcoords='offset points')\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2nd Approach with NLTK (VADER)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "used_approach = \"NLTK_VADER\" #setting it for later reference\n",
    "\n",
    "# Initialize the SentimentIntensityAnalyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Apply sentiment analysis and store the compound score in a new column\n",
    "df_nltk['NLTK_Sentiment'] = df_nltk['Cleaned_Text'].apply(lambda text: sia.polarity_scores(text)['compound'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Apply sentiment analysis and store the compound score in a new column\n",
    "\n",
    "# Record the start time\n",
    "start_time = time.time()\n",
    "\n",
    "for i, row in enumerate(df_nltk.index):\n",
    "    text = df_nltk.at[row, 'Cleaned_Text']\n",
    "    sentiment_score = sia.polarity_scores(text)['compound']\n",
    "    df_nltk.at[row, 'NLTK_Sentiment'] = sentiment_score\n",
    "\n",
    "    # Print progress message\n",
    "    print(f\"Text number {i + 1} out of all {total_rows} rows in the input file: Sentiment score = {sentiment_score}.\")\n",
    "\n",
    "# Calculate and print the elapsed time\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(\"-\"*100)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# End information about the run\n",
    "print(f\"The used approach #2 was {used_approach}.\")\n",
    "print(f\"Elapsed time: {elapsed_time:.0f} seconds.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Saving the output in two file formats in dedicated files for only this current approach in the correct repository folder\n",
    "\n",
    "# Save the results to an Excel file\n",
    "excel_output_file_nltk = os.path.join(output_folder, 'sentiment_results_nltk.xlsx')\n",
    "df_nltk.to_excel(excel_output_file_nltk, index=False)\n",
    "\n",
    "# Save the results to a csv file with the same name\n",
    "csv_output_file_nltk = os.path.join(output_folder, 'sentiment_results_nltk.csv')\n",
    "df_nltk.to_csv(csv_output_file_nltk, index=False, encoding='utf-8', quoting=csv.QUOTE_ALL)\n",
    "\n",
    "# Print final info\n",
    "print(f\"Results saved as {csv_output_file_nltk} and {excel_output_file_nltk}.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Explorative Analysis of NLTK (VADER)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Calculate and print statistics for 'NLTK_Sentiment_Score' in df_nltk\n",
    "print(\"Statistics for NLTK Sentiment Score:\")\n",
    "\n",
    "# Average Score\n",
    "print(\"\\nAverage Score:\")\n",
    "print(df_nltk['NLTK_Sentiment'].mean())\n",
    "\n",
    "# Minimum Score\n",
    "print(\"\\nMinimum Score:\")\n",
    "print(df_nltk['NLTK_Sentiment'].min())\n",
    "\n",
    "# Maximum Score\n",
    "print(\"\\nMaximum Score:\")\n",
    "print(df_nltk['NLTK_Sentiment'].max())\n",
    "\n",
    "# Standard Deviation\n",
    "print(\"\\nStandard Deviation:\")\n",
    "print(df_nltk['NLTK_Sentiment'].std())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Check some results of NLTK (VADER)\n",
    "\n",
    "# Sort the DataFrame by sentiment scores (positive to negative)\n",
    "df_nltk_sorted = df_nltk.sort_values(by='NLTK_Sentiment', ascending=False)\n",
    "\n",
    "# Select the 5 most positive entries\n",
    "most_positive_entries = df_nltk_sorted.head(5)\n",
    "\n",
    "# Select the 5 most negative entries\n",
    "most_negative_entries = df_nltk_sorted.tail(5)\n",
    "\n",
    "# Select 5 entries with exactly neutral scores (around 0)\n",
    "neutral_entries = df_nltk_sorted[(df_nltk_sorted['NLTK_Sentiment'] >= -0.1) & (df_nltk_sorted['NLTK_Sentiment'] <= 0.1)].head(5)\n",
    "\n",
    "# Concatenate the selected entries\n",
    "selected_entries = pd.concat([most_positive_entries, most_negative_entries, neutral_entries])\n",
    "\n",
    "# Display all columns for the selected entries\n",
    "print(selected_entries)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create a histogram to visualize the distribution of sentiment scores\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.hist(df_nltk['NLTK_Sentiment'], bins=20, color='skyblue', edgecolor='black')\n",
    "plt.title('Distribution of NLTK VADER Sentiment Scores')\n",
    "plt.xlabel('Sentiment Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3rd Approach with BERT"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Short intro\n",
    "**B**idirectional **E**ncoder **R**epresentations from **T**ransformers (**BERT**) is a family of language models introduced in 2018 by researchers at Google.\n",
    "BERT is an \"encoder-only\" transformer architecture.\n",
    "\n",
    "On a high level, BERT consists of three modules:\n",
    "\n",
    "- **embedding**. This module converts an array of one-hot encoded tokens into an array of vectors representing the tokens.\n",
    "- a **stack of encoders**. These encoders are the Transformer encoders. They perform transformations over the array of representation vectors.\n",
    "- **un-embedding**. This module converts the final representation vectors into one-hot encoded tokens again.\n",
    "\n",
    "There are different BERT models available out there.\n",
    "\n",
    "**BERT-Large (e.g., bert-large-uncased):**\n",
    "- one of the largest versions of BERT, with more parameters, making it capable of handling longer text\n",
    "- high accuracy; can capture detailed context\n",
    "- requires substantial computational resources and may be slower to run\n",
    "\n",
    "**RoBERTa (e.g., roberta-large):**\n",
    "- similar to BERT, but fine-tuned for better performance\n",
    "- has been optimized for efficiency, can handle longer text as well\n",
    "- often performs at par with BERT-Large but with lower computational requirements\n",
    "\n",
    "**Longformer (e.g., allenai/longformer-large-4096):**\n",
    "- designed specifically for handling long documents and sequences\n",
    "- well-suited for tasks involving lengthy text, like news articles\n",
    "- significantly extends the maximum token limit (e.g., up to 4096 tokens) compared to traditional BERT models\n",
    "\n",
    "**Electra (e.g., google/electra-large-discriminator):**\n",
    "- efficient and performs well on various NLP tasks\n",
    "- not explicitly designed for long text, but it can still be used for document-level sentiment analysis effectively\n",
    "\n",
    "**DistilBERT (e.g., distilbert-base-uncased):**\n",
    "- smaller and more efficient version of BERT.\n",
    "- may have limitations in handling very long texts due to its reduced model size\n",
    "\n",
    "#### Result\n",
    "BERT-Large might be used if resources can handle it. If not, RoBERTa will receive a try.\n",
    "First try however, due to it being suited for news texts is Longformer.\n",
    "Electra and DistilBERT don't appear to be good choices."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Using BERT-Large"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load the BERT-Large model and tokenizer\n",
    "model_name = 'bert-large-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForSequenceClassification.from_pretrained(model_name)\n",
    "model.eval()\n",
    "\n",
    "# Create a list to store the results\n",
    "sentiments = []\n",
    "\n",
    "# Get the total number of rows in the DataFrame\n",
    "total_rows = len(df_bert_large)\n",
    "\n",
    "# Loop through each row in the DataFrame\n",
    "for current_row, (index, row) in enumerate(df_bert_large.iterrows(), start=1):\n",
    "    text = row['Cleaned_Text']\n",
    "\n",
    "    # Pre-process the text\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True)\n",
    "\n",
    "    # Perform sentiment analysis\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        probabilities = softmax(logits, dim=1)\n",
    "\n",
    "    # Extract the predicted sentiment label\n",
    "    predicted_label = torch.argmax(probabilities).item()\n",
    "    sentiment_labels = [\"Negative\", \"Neutral\", \"Positive\"]  # Customize as needed\n",
    "    predicted_sentiment = sentiment_labels[predicted_label]\n",
    "\n",
    "    # Append the result to the list\n",
    "    sentiments.append(predicted_sentiment)\n",
    "\n",
    "    # Print progress update\n",
    "    print(f\"Analyzing text number {current_row} out of {total_rows} rows in the input file.\")\n",
    "\n",
    "# Add the sentiments as a new column to the DataFrame\n",
    "df_bert_large['Predicted_Sentiment'] = sentiments\n",
    "\n",
    "# Print the DataFrame with the added sentiment column\n",
    "print(df_bert_large)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Explorative Analysis of BERT"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
