{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Topic Modeling"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b05a233d4a098ecc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1) Load Data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4bd351a9c0b8fe78"
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "# All Imports\n",
    "import numpy as np  \n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-10T19:47:14.734611Z",
     "start_time": "2023-10-10T19:47:14.619328Z"
    }
   },
   "id": "ce896e161a772e73"
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_csv(r'/Users/nicohehlke/DataspellProjects/NAK-Text-Analytics/src/data/transfer/cleaned_articles_normalverteilt_3.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-10T19:47:20.894043Z",
     "start_time": "2023-10-10T19:47:16.631203Z"
    }
   },
   "id": "2e4aeb9ff2186942"
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6.96378830e-05 1.04877690e-02 6.96378830e-05 ... 6.96378830e-05\n",
      "  6.96378830e-05 6.96378830e-05]\n",
      " [2.47524752e-04 2.47524752e-04 2.47524752e-04 ... 2.47524752e-04\n",
      "  2.47524752e-04 2.47524752e-04]\n",
      " [3.83435583e-05 9.01575849e-05 3.83435583e-05 ... 3.83435583e-05\n",
      "  3.83435583e-05 5.91433085e-03]\n",
      " ...\n",
      " [1.08225108e-04 1.08225108e-04 1.08225108e-04 ... 1.08225108e-04\n",
      "  1.08225108e-04 1.08225108e-04]\n",
      " [3.52112676e-04 3.52112676e-04 3.52112676e-04 ... 3.52112676e-04\n",
      "  3.22678185e-01 3.52112676e-04]\n",
      " [2.77771976e-02 5.86854460e-05 5.86854460e-05 ... 5.86854460e-05\n",
      "  9.02255982e-02 5.86854460e-05]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create a list of all texts\n",
    "texts = df[\"Cleaned_Text\"].tolist()\n",
    "\"\"\"\n",
    "# Transform texts to tf-idf-values\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(texts)\n",
    "\"\"\"\n",
    "# Calculate the term frequency\n",
    "vectorizer = CountVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "# Create LDA-model\n",
    "lda_model = LatentDirichletAllocation(n_components=40, random_state=1)\n",
    "lda_topic_matrix = lda_model.fit_transform(tfidf_matrix)\n",
    "print(lda_topic_matrix)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-10T20:04:07.514773Z",
     "start_time": "2023-10-10T19:47:34.556960Z"
    }
   },
   "id": "55395777d4d1fbf3"
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n",
      "                                            Cleaned_Text  dominant_topic\n",
      "0      neu Rumor Porschebörsengang Sixt Berenberg stu...              18\n",
      "1      Beiersdorf Aktie Kaufempfehlung beflügeln Bere...               5\n",
      "2      Heidelbergcement klimaneutral Zementwerk Weg B...              36\n",
      "3      Dax bleiben Rekordhoch Vortag lustlos setzen R...              16\n",
      "4      Sartorius Impferfolg belasten Papier Laborausr...              23\n",
      "...                                                  ...             ...\n",
      "38516  Eqscms Siemens Aktiengesellschaft Veröffentlic...              28\n",
      "38517  national Luftfahrtkonferenz Hamburg Kanzler Sc...              31\n",
      "38518  Stiebeleltronwärmepumpe Gifhorn Contijob erhal...              23\n",
      "38519  national Luftfahrtkonferenz Hamburg Kanzler Sc...              31\n",
      "38520  Dpaafxüberblick Unternehmen Uhr Roundupvor Woh...              36\n",
      "\n",
      "[38521 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# What topic is relevant for each text?\n",
    "dominant_topics = [np.argmax(topic) for topic in lda_topic_matrix]\n",
    "print(dominant_topics[0])\n",
    "# Add a new column for the topic\n",
    "df['dominant_topic'] = dominant_topics\n",
    "print(df[['Cleaned_Text', 'dominant_topic']])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-10T20:04:57.079936Z",
     "start_time": "2023-10-10T20:04:53.712588Z"
    }
   },
   "id": "ff49fa3013b39068"
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['bank', 'deutsch', 'credit', 'suisse', 'jahr'], ['unternehmen', 'prozent', 'jahr', 'deutsch', 'deutschland'], ['urteil', 'bgh', 'eugh', 'schadenersatz', 'auto'], ['prozent', 'jahr', 'china', 'auto', 'bmw'], ['aufsichtsrat', 'gesellschaft', 'hauptversammlung', 'vergütung', 'vorstand'], ['prozent', 'aktie', 'beachten', 'erhalten', 'bedingung'], ['q3zahlen', 'deu', 'unternehmen', 'q3umsatz', 'termin'], ['ziel', 'eur', 'senken', 'heben', 'deu'], ['daimler', 'truck', 'zalando', 'puma', 'holding'], ['wphg', 'stimmrecht', 'goldman', 'sachs', 'summe'], ['euro', 'milliarde', 'jahr', 'prozent', 'quartal'], ['dax', 'onvista', 'inhalt', 'deutsch', 'information'], ['fresenius', 'fmc', 'medical', 'care', 'unternehmen'], ['aktie', 'jahr', 'euro', 'unternehmen', 'fool'], ['china', 'sagen', 'lützerath', 'polizei', 'deutsch'], ['euro', 'dpaafx', 'originalstudie', 'studie', 'broker'], ['prozent', 'punkt', 'dax', 'aktie', 'deutsch'], ['dax', 'deutsch', 'neu', 'woche', 'stehen'], ['prozent', 'dax', 'punkt', 'aktie', 'deutsch'], ['bayer', 'sap', 'jahr', 'unternehmen', 'wellington'], ['atomkraftwerk', 'deutsch', 'sagen', 'dpaafx', 'dax'], ['produkt', 'information', 'wertpapier', 'risiko', 'wertentwicklung'], ['eur', 'aktie', 'stock3', 'autor', 'aktuell'], ['euro', 'prozent', 'jahr', 'umsatz', 'quartal'], ['deu', 'usa', 'q2zahlen', 'halbjahreszahlen', 'pmi'], ['bank', 'prozent', 'commerzbank', 'jahr', 'deutsch'], ['blackrock', 'inc', 'limited', 'holdco', 'holdings'], ['sagen', 'unternehmen', 'euro', 'deutsch', 'deutschland'], ['aktie', 'abs', 'veröffentlichung', 'eqs', 'verordnung'], ['adler', 'uniper', 'gericht', 'sagen', 'unternehmen'], ['hurrikan', 'sagen', 'mensch', 'ian', 'florida'], ['sagen', 'deutschland', 'neu', 'eukommission', 'berlin'], ['porsche', 'euro', 'volkswagen', 'prozent', 'börsengang'], ['eur', 'geschäft', 'person', 'preis', 'angabe'], ['infineon', 'prozent', 'neu', 'jahr', 'unternehmen'], ['prozent', 'aktie', 'dax', 'deutsch', 'punkt'], ['russland', 'ukraine', 'russisch', 'sagen', 'deutschland'], ['prozent', 'jahr', 'sagen', 'post', 'deutschland'], ['airbus', 'jahr', 'boeing', 'maschine', 'roundup'], ['siemens', 'euro', 'energy', 'prozent', 'gamesa']]\n"
     ]
    }
   ],
   "source": [
    "# Extract the most important word for each topic\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "topic_words = []\n",
    "for idx, topic in enumerate(lda_model.components_):\n",
    "    words = [feature_names[i] for i in topic.argsort()[:-5 - 1:-1]]\n",
    "    topic_words.append(words)\n",
    "    #print(sorted(topic, reverse=True)[:3])\n",
    "    #Was ist, wenn es das wichtigste WOrt schon gibt? Dann das 2. Wichtigste nehmen? ist aber ja eigentlich falsch\n",
    "print(topic_words)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-10T20:05:19.139746Z",
     "start_time": "2023-10-10T20:05:16.902809Z"
    }
   },
   "id": "811f0781c4b06a87"
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 8 36  1 ...  1 36  1]\n"
     ]
    }
   ],
   "source": [
    "# Create topic-clusters with kMeans\n",
    "num_clusters = 40  # e.g. one cluster for each topic\n",
    "km = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "km.fit(tfidf_matrix)\n",
    "\n",
    "# Predict cluster for each text\n",
    "texte_cluster = km.predict(tfidf_matrix)\n",
    "print(texte_cluster)\n",
    "\n",
    "# Mapping between topics and cluster\n",
    "themen_cluster_mapping = {}\n",
    "for idx, cluster in enumerate(texte_cluster): #z.B. Index 0 = erstes Cluster = \"5\" = erstes dominant_topics = topic \"16\"\n",
    "    topic_per_text = dominant_topics[idx]  # dominant topics for each text\n",
    "    if topic_per_text not in themen_cluster_mapping:\n",
    "        themen_cluster_mapping[topic_per_text] = cluster\n",
    "        \n",
    "# Label the topic for each text based on the cluster\n",
    "automatic_labels = [topic_words[themen_cluster_mapping[thema]][0] for thema in dominant_topics]\n",
    "df[\"topic\"] = automatic_labels\n",
    "#df.to_csv('themen.csv', index=False)\n",
    "# Exportiere den DataFrame als Excel-Datei\n",
    "df.to_excel('data//transfer//topic_modeling_v1.xlsx', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-10T20:35:26.347881Z",
     "start_time": "2023-10-10T20:32:08.612530Z"
    }
   },
   "id": "b44d414f1bfcf681"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Baustelle"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "908cca02f730e21a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim import corpora, models\n",
    "\n",
    "# Tokenisierung und Entfernung von Stoppwörtern\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('german'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    words = word_tokenize(text.lower())\n",
    "    filtered_words = [word for word in words if word.isalnum() and word not in stop_words]\n",
    "    return filtered_words\n",
    "\n",
    "# Tokenisierte Texte vorbereiten\n",
    "tokenized_texts = [preprocess_text(text) for text in df['Text']]\n",
    "\n",
    "# Wort-Dictionary erstellen\n",
    "dictionary = corpora.Dictionary(tokenized_texts)\n",
    "\n",
    "# Texte in ein Bag-of-Words-Format umwandeln\n",
    "corpus = [dictionary.doc2bow(text) for text in tokenized_texts]\n",
    "\n",
    "# LDA-Modell erstellen\n",
    "lda_model = models.LdaModel(corpus, num_topics=20, id2word=dictionary, passes=15)\n",
    "\n",
    "# Funktion zum Zuordnen eines Textes zu einem Thema\n",
    "def assign_topic(text):\n",
    "    bow = dictionary.doc2bow(preprocess_text(text))\n",
    "    topic_probs = lda_model.get_document_topics(bow)\n",
    "    topic_probs = sorted(topic_probs, key=lambda x: x[1], reverse=True)  # Sortieren nach Wahrscheinlichkeit\n",
    "    return topic_probs[0][0]  # Index des wahrscheinlichsten Themas\n",
    "\n",
    "# Neue Spalte 'Thema' im DataFrame erstellen\n",
    "df['Thema'] = df['Text'].apply(assign_topic)\n",
    "\n",
    "# Ausgabe des DataFrames mit der neuen 'Thema'-Spalte\n",
    "print(df[['Thema', 'Text']])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6a9ad1ecf5e37689"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Versuch"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "92fba920b37b386d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Import the wordcloud library\n",
    "from wordcloud import WordCloud\n",
    "# Join the different processed titles together.\n",
    "long_string = ','.join(list(df['Text'].values))\n",
    "# Create a WordCloud object\n",
    "wordcloud = WordCloud(background_color=\"white\", max_words=5000, contour_width=3, contour_color='steelblue')\n",
    "# Generate a word cloud\n",
    "wordcloud.generate(long_string)\n",
    "# Visualize the word cloud\n",
    "wordcloud.to_image()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2311f4b12562c89"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('german')\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        # deacc=True removes punctuations\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc))\n",
    "             if word not in stop_words] for doc in texts]\n",
    "data = df.text_processed.values.tolist()\n",
    "data_words = list(sent_to_words(data))\n",
    "# remove stop words\n",
    "data_words = remove_stopwords(data_words)\n",
    "print(data_words[:1][0][:30])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1b60d66e48d2bc3c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import gensim.corpora as corpora\n",
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_words)\n",
    "# Create Corpus\n",
    "texts = data_words\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "# View\n",
    "print(corpus[:1][0][:30])\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6cf33b74ac7fca32"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
